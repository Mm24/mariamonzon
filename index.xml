<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Maria Monzon</title>
    <link>https://mariamonzon.com/</link>
      <atom:link href="https://mariamonzon.com/index.xml" rel="self" type="application/rss+xml" />
    <description>Maria Monzon</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://mariamonzon.com/media/icon_huaad826f52d8c51d03abf0418cbb027d5_448596_512x512_fill_lanczos_center_3.png</url>
      <title>Maria Monzon</title>
      <link>https://mariamonzon.com/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>https://mariamonzon.com/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/talk/example-talk/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Panoramic based 3D-Viewer</title>
      <link>https://mariamonzon.com/project/3d-viewer/</link>
      <pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/project/3d-viewer/</guid>
      <description>&lt;p&gt;Application designed to designed to enable the computer science department of the Friedrich-Alexander University Erlangen-Nuremberg to display its 50 years computer science department exhibition online. This 3D Viewer web application can be easily accessed via &lt;a href=&#34;https://50-jahre-informatik.cs.fau.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://50-jahre-informatik.cs.fau.de&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/3d-viewer/example-viewer.png&#34; alt=&#34;Visualization&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The mission of the project was to deliver a web based viewer that allows users to display the
panoramas of the computer science department&amp;rsquo;s 50th anniversary on the web, without
having to pay extensive license costs for other commercial 3D viewers. Detailed
information at each booth should be delivered via third-party plugins, which operate on the
viewers API.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a web-based Viewer in order to display the exhibition from the 50th
anniversary of the CS department at FAU&lt;/li&gt;
&lt;li&gt;Users can rotate the view, zoom in/out, walk through rooms and change
floors&lt;/li&gt;
&lt;li&gt;an extensive API is provided such that third party plugins can be integrated&lt;/li&gt;
&lt;li&gt;A map is integrated in the bottom corner of the screen, such that the user
always has a feeling of where is currently standing inside the room&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/3d-viewer/start-page.png&#34; alt=&#34;Start-Page&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fully automatic extraction of mitral valve annulus motion parameters on long axis CINE CMR using deep learning</title>
      <link>https://mariamonzon.com/publication/ismrm-motion-parameters/</link>
      <pubDate>Thu, 20 May 2021 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/publication/ismrm-motion-parameters/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>Automated Vessel Segmentation for 2D Phase Contrast MR Using Deep Learning</title>
      <link>https://mariamonzon.com/publication/ismrm-segmentation/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/publication/ismrm-segmentation/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning based reach-and-grasp EEG decoder</title>
      <link>https://mariamonzon.com/project/eeg-grasp-decoder/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/project/eeg-grasp-decoder/</guid>
      <description>&lt;p&gt;Decoding three different executed reach-and-grasp actions utilizing their electroencephalogram (EEG) recording from different electrodes is of crutial significance for the rehabilitation of hand functions of patients with motor disorders &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/28853420/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;.
Despite the high freedom of the human hand movements, most actions of daily life can be executed incorporating only palmar, lateral and grasp.  Recent studies have already shown that neural correlates of natural reach-and-grasp actions can be identified in the EEG &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/28853420/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2020.00849/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Deep Learning has recently achieved promising results in the field of Computer Vision and Biomedical Engineering. Therefore, this work aims to study the possibility of develop Deep learning based decoders to classify grasp actions based on EEG signals.  We have also studied the possibility of developing intersubject classifiers and transfer learning between the different subject technologies. For this purpose, different neural network architectures have been tested, single trial vs crop trial performance has been evaluated as well as the different training techniques: within-subject and inter-subject training.&lt;/p&gt;
&lt;h3 id=&#34;1-eeg-introduction&#34;&gt;1-EEG Introduction&lt;/h3&gt;
&lt;p&gt;The EEG is a cost-effective, non-invasive technique to examine brain activity linked to multiple neurocognitive processes that underlie human behavior. It consists of placing electrodes on the head to monitor the electrical activity produced when neurons fire. The EEG records and measures electrical signals of the human brain from multiple cortical areas. Therefore, EEG monitoring allows to quantify different types of brain waves, also known as neural oscillations.
The standard pipeline followed to extract information is depicted in the next figure:&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-eeg-standard-decoding-pipeline-adapted-from-bitbrain-three-important-steps-when-processing-eeg-april-23-2020-httpswwwbitbraincomblogai-eeg-data-processing&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/eeg-grasp-decoder/methods-eeg-decoding.png&#34; alt=&#34;methods-eeg-decoding&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      EEG standard decoding pipeline. Adapted from 
Bitbrain. “Three important steps when processing EEG”, April 23, 2020, &lt;a href=&#34;https://www.bitbrain.com/blog/ai-eeg-data-processing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.bitbrain.com/blog/ai-eeg-data-processing&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;2-dataset&#34;&gt;2-Dataset&lt;/h3&gt;
&lt;p&gt;In a cue-guided experiment, 15 healthy individuals were asked to perform reach-and-grasp actions using daily life objects.  The dataset is publicly available at &lt;a href=&#34;http://bnci-horizon-2020.eu/database/data-sets&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BNCI Horizon 2020&lt;/a&gt;. 
The pre-recorded dataset contains 7 min runs, leading to 80 trials per condition (TPC) distributed over 4 runs / 20 trials for each reach-and-grasp condition and from a no-movement condition.
The 45 right handed participants performed two self-initiated reach-and-grasp (palmar and lateral grasp) movement conditions.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-reach-and-grasp-movement-decoding-from-eeg-with-gel-water-and-dry-electrodes-dataset-experimental-set-up-2httpswwwfrontiersinorgarticles103389fnins202000849full&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/eeg-grasp-decoder/grasp-dataset-definition.png&#34; alt=&#34;grasp-task&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Reach and Grasp movement decoding from EEG with gel, water and dry electrodes dataset experimental set-up &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2020.00849/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gel-based electrodes recordings. EEG was measured with 58 electrodes (frontal, central and parietal areas).&lt;/li&gt;
&lt;li&gt;Water-based electrodes recordings mobile and water-based electrodes EEG-Versatile™ system with 32 electrodes&lt;/li&gt;
&lt;li&gt;Dry-electrodes recordings measured using the dry-electrodes EEG-Hero™ headset.  EEG was measured with 11 electrodes over the sensorimotor cortex.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;data-proprocesssing&#34;&gt;Data Proprocesssing&lt;/h4&gt;
&lt;p&gt;The EEG data processsing was analogous to the one in &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2020.00849/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt;. All the modalities data were filtered with a zero-phase 4th order Butterworth filter with a cut-off frequency of 0.3 and resample to 128 Hz. 
We defined a window of interest for each movement trial of [-2 3] s with respect to the movement onset at second 0.
In addition, we also extracted 81 rest trials from inactivity periods with a duration of 5 seconds.&lt;/p&gt;
&lt;h3 id=&#34;3-methods&#34;&gt;3-Methods&lt;/h3&gt;
&lt;h4 id=&#34;vanilla-1d-network&#34;&gt;Vanilla 1D Network&lt;/h4&gt;
&lt;p&gt;We aimed to design a single convolutional neural network (CNN) architecture to accurately classify grasp actions from differente EEG decoding modalitites, while  being as compact and simple as possible. We try a simple vanilla 1D convolutional neural network based on 1D temporal convolutions in order to encapsulate EEG feature extraction methodologies used in traditional classiers &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2020.00849/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt;
















&lt;figure  id=&#34;figure-overview-of-1d-cnn-designed-architecture-it-contains-a-1d-convolution-block-layer-followed-by-a-temporal-pooling-and-convolution-kernel-to-extract-features-that-are-the-input-for-the-dense-layer&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/eeg-grasp-decoder/network-description.png&#34; alt=&#34;methods-eeg-network&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Overview of 1D CNN designed architecture. It contains a 1D convolution block layer followed by a temporal pooling and convolution kernel to extract features that are the input for the dense layer
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;eegnet-3httpsiopscienceioporgarticle1010881741-2552aace8c&#34;&gt;EEGNet &lt;a href=&#34;https://iopscience.iop.org/article/10.1088/1741-2552/aace8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[3]&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;EEGNet is a compact CNN designed for BCIs that can be trained with very limited data. The architecture has three convolution layers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a one-dimensional convolution analogous to temporal band-pass filtering&lt;/li&gt;
&lt;li&gt;a depthwise convolution to perform spatial filtering,&lt;/li&gt;
&lt;li&gt;a separable convolution to identify temporal patterns across the previous filters&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;htnet-architecture-4httpsdoiorg1010881741-2552abda0b&#34;&gt;HTNet architecture &lt;a href=&#34;https://doi.org/10.1088/1741-2552/abda0b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[4]&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;HTNet builds upon EEGNet &lt;a href=&#34;https://iopscience.iop.org/article/10.1088/1741-2552/aace8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[3]&lt;/a&gt;. The authors added a Hilbert transform layer after this initial temporal convolution to compute relevant spectral power features using a data-driven filter-Hilbert.  The temporal convolution and Hilbert transform layers generate data-driven spectral features that can then be projected from electrodes onto common regions of interest using a predefined weight matrix.&lt;/p&gt;
&lt;h4 id=&#34;training-strategies&#34;&gt;Training Strategies&lt;/h4&gt;
&lt;p&gt;Transfer learning techniques from the field of machine learning have been adopted also for EEG feature distribution for inter-subject variability. The common cross-validation strategy used in EEG decoding is known as &amp;ldquo;leave one-subject-out&amp;rdquo;. Given the N subjects, the training subset is fromed by N - 1, while the remaining subject is used for testing.
Classiffcation results are reported for differente training stratesgies: within-subject, inter-subject and with pretraining in another recording technology.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-test-strategies-for-the-evaluation-of-the-resutls&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/eeg-grasp-decoder/methods-training-valdation-strategy.png&#34; alt=&#34;methods-training&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Test strategies for the evaluation of the resutls
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;data-augmentation&#34;&gt;Data Augmentation&lt;/h4&gt;
&lt;p&gt;Data augmentation refers to techniques used to increase the amount of data by slightly modifying training data. 
Data augmentation is especially useful for EEG signals where the limitation of small-scale datasets greatly affects the performance of classifiers. Still due to the variability of EEG and time-series nature, it is challenging to augment the data in the feature space. Based on the findings of &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2020.00849/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt;, we implemented an easy on-the-fly data augmentation that consist on band filtering the training data. The aim is to enforce the network to learn different features at different frequency bands.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-overview-of-training-pipeline-including-the-data-augmentation-method&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/eeg-grasp-decoder/methods-split-bands.png&#34; alt=&#34;methods-augmentation&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Overview of training pipeline including the data augmentation method
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;4-results&#34;&gt;4-Results&lt;/h3&gt;
&lt;p&gt;In a single-trial multiclass based decoding approach, which incorporated both movement conditions and rest
can be successfully decodes using Deep learning based decoders. 
We performed a comparison on the decoding accuracy for single trial of 2 seconds on the state-of-the-art network architecture on the time of the study.  Table depicts the inter-participant classification results.
















&lt;figure  id=&#34;figure-decoding-accuracy-on-single-trial-classification&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/eeg-grasp-decoder/results-architecture.png&#34; alt=&#34;results-architecture&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Decoding accuracy on single trial classification
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The effect of cropping window duration was investigated. We can conlcude that longer signal windows show better performance.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-overview-of-training-pipeline-including-the-data-augmentation-method&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/eeg-grasp-decoder/results-cropping-window.png&#34; alt=&#34;results-window&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Overview of training pipeline including the data augmentation method
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The best results were achieved when training the model with a cropped window T =[0,1] with overlapping strides 250ms. The models were pretrained in another recording technology resampled to 128 Hz with split frequencies data augmentation strategy. On average, best classification performance could be reached 1s after the movement onset.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-single-trial-decoding-performance-of-the-dry--and-water-electrodes-recordings&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/eeg-grasp-decoder/results-accuracy-time.png&#34; alt=&#34;results-accuracy&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Single trial decoding performance of the dry- and water-electrodes recordings
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Despite the reduced number of channels of the dry electrodes recordings, the average performance was not decreased significantly as it can be seen in the above figure.&lt;/p&gt;
&lt;h4 id=&#34;5-conclusion&#34;&gt;5-Conclusion&lt;/h4&gt;
&lt;p&gt;This study confirmed that EEG based correlates of reach-and-grasp actions can be successfully identified  using Deep Leaning based decoders. We demonstrated that a simple, yet effective,  1D convolution CNN  can reach state-of-the-art neural decoders when and improve the results appliying the mmodes  to new participants, even when a different recording modality is used. Unfortunately, a direct comparison to other reach-and-grasp studies such as is difficult due significant differences in experimental setup and paradigm and hence cannot be made in a serious manner.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/28853420/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt; Schwarz A, Ofner P, Pereira J, Sburlea AI, Müller-Putz GR. Decoding natural reach-and-grasp actions from human EEG. J Neural Eng. 2018 Feb;15(1):016005. doi: 10.1088/1741-2552/aa8911. PMID: 28853420.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2020.00849/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt; Schwarz, A., Escolano, C., Montesano, L., &amp;amp; Müller-Putz, G. (2020). Analyzing and Decoding Natural Reach-and-Grasp Actions Using Gel, Water and Dry EEG Systems. Frontiers in Neuroscience, 14.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://iopscience.iop.org/article/10.1088/1741-2552/aace8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[3]&lt;/a&gt; Lawhern V J, Solon A J, Waytowich N R, Gordon S M, Hung C P and Lance B J 2018 Eegnet: a compact convolutional neural network for EEG-based brain–computer interfaces J. Neural Eng. 15 056013&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.1088/1741-2552/abda0b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[4]&lt;/a&gt; Peterson, S. M., Steine-Hanson, Z., Davis, N., Rao, R. P. N., &amp;amp; Brunton, B. W. (2021). Generalized neural decoders for transfer learning across participants and recording modalities.
Journal of Neural Engineering. &lt;a href=&#34;https://doi.org/10.1088/1741-2552/abda0b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1088/1741-2552/abda0b&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fully automatic extraction of mitral valve annulus motion parameters on long axis CINE CMR using deep learning</title>
      <link>https://mariamonzon.com/project/cardiac-parameter-extraction/</link>
      <pubDate>Thu, 31 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/project/cardiac-parameter-extraction/</guid>
      <description>&lt;p&gt;Diastolic dysfunction is an important cause of cardiac insuf-
ficiency, defined as a malfunctioning filling of the heart during diastole.
The analysis of mitral valve motion is known to be relevant in the diagno-
sis of cardiac dysfunction. Cardiac motion parameters can be extracted
from Cardiac Magnetic Resonance (CMR) images. However, in clinical
setting valve motion modeling usually needs a manual intervention to lo-
calize the valvular plane. We propose two chained Convolutional Neural
Networks (CNN) for automatic tracking of mitral valve-annulus land-
marks on time-resolved 2-chamber and 4-chamber CMR images. 
The first CNN is trained to detect the region of interest and the second to
track the landmarks along the cardiac cycle. The presented deep learn-
ing system has high accuracy in terms of temporal landmark tracking
and motion assessment. Furthermore, we successfully extracted several
motion-related parameters thereby overcoming time-consuming annota-
tion and allowing statistical analysis over a large number of datasets.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Warehouse storing route optimization with Reinforcement Learning</title>
      <link>https://mariamonzon.com/project/reinforcement-learning-warehouse/</link>
      <pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/project/reinforcement-learning-warehouse/</guid>
      <description>&lt;p&gt;The problem represents a storage decision process where outcome is under the control of a robot, i.e, a decision maker
agent, but also are partly random. Therefore, the problem can be well modeled as a discrete-time Markov Decision
Processes. The approach follow was:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implement a reinforcement-learning based algorithm&lt;/li&gt;
&lt;li&gt;The robot is the agent and decides where to place the next part&lt;/li&gt;
&lt;li&gt;Use the markov decision process toolbox for your solution&lt;/li&gt;
&lt;li&gt;Choose the best performing MDP&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;defitinions&#34;&gt;Defitinions&lt;/h2&gt;
&lt;p&gt;The basic concepts to understand the promblem are shortly introduced based on &lt;em&gt;Artificial Intelligence: A Modern Approach&lt;/em&gt; book:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reinforcement learning&lt;/strong&gt;: Type of dynamic programming that trains algorithms using
a system of reward and punishment. The agent learns without intervention from a human by maximizing its reward
and minimizing its penalty and updates itself continuously. The algorithm it automatically finds patterns
and relationships inside of that dataset. It requires realtime data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Agent&lt;/strong&gt;: A is the set of all possible moves the agent can make. An action is almost self-explanatory, but it should be noted that agents choose among a list of possible actions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Environment&lt;/strong&gt;: The world through which the agent moves. The environment takes the agent&amp;rsquo;s current state and action as input, and returns as output the agent&amp;rsquo;s reward and its next state.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;State&lt;/strong&gt;: The parameter values that describe the current cofiguration of the environment, which the agent uses to choose an action. A state is a concrete and immediate situation in which the agent finds itself; i.e. a specific place and moment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reward&lt;/strong&gt;: Feedback by which we effectively evaluate the agent&amp;rsquo;s action.
From any given state, an agent sends output in the form of actions to the environment, and the environment
returns the agent&amp;rsquo;s new state (which resulted from actingon the previous state) as well as rewards, if any.
Rewards
can be immediate or delayed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Markov decision process&lt;/strong&gt;: Markov decision processes (MDPS) is a model decision making in stochastic, sequential environments. The essence of the model is that a decision maker, or agent, inhabits an environment, which changes state randomly in response to action choices made by the agent. 
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/reinforcement-learning-warehouse/agent-environment.png&#34; alt=&#34;agent&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-introduction&#34;&gt;Example Introduction&lt;/h2&gt;
&lt;p&gt;A tiny warehouse with (2x2) storage capacity locations is simulated. The picking robot (agent) interacts with the
warehouse (environment) by store-restoring items in each warehouse cell or shelve. The dataset contains store and
restore action for red, blue and white colored items although an empty field is also possible.&lt;/p&gt;
&lt;p&gt;When the agent is placed on a field position (𝑥𝑑, 𝑦𝑑), it can either store or restore each of the color items.
There exists a total of six possible actions to change its environment. The robot can move in the (2x2) grid
environment and start moving at initial grid position (1,1) . The robot is constrained to always to move only
to one adjacent fields (not in diagonal).&lt;/p&gt;
&lt;p&gt;The distance the robot needs to move is derived based on current (𝑥𝑐 , 𝑦𝑐 ) and goal position (𝑥𝑑 , 𝑦𝑑). The
distance is calculated as the sum of the absolute differences of the layout position

$$ 𝑑 = |𝑥_𝑑 − 𝑥_𝑐 | + |𝑦_𝑑 − 𝑦_𝑑|$$ 
&lt;/p&gt;
&lt;p&gt;Therefore, the distance to the position can be assigned to the cost of the action or negative reward. The ideal
goal of the reinforcement learning approach would be to optimize the route picking storage strategy by
minimizing the distance with rewards the store/restore motion actions.&lt;/p&gt;
&lt;h2 id=&#34;methods&#34;&gt;Methods&lt;/h2&gt;
&lt;p&gt;The Markov Discrete Process (MDP) algorithm was implemented for modelling the problem. A MDP is
described by state transition probabilities. A general Reinforcement Learning and thus also MDP algorithm
is defined with a set of variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Actions (A): refers to the operations an agent can perform which direct modify the environment. In our
problem this are related to the effect on the warehouse grid cells, directly proportional to the warehouse size
(𝑋, 𝑌). $ A = \{ 𝐴_{(1,1)}, 𝐴_{(1,2)}, 𝐴_{(2,1)}, … , 𝐴_{(X,Y)}\}$  where  $ |𝐴| = |𝑋 \cdot 𝑌|$ .&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;States (S): represent all the possible configuration of environment, i.e, how the items are storage in the warehouse grid. In the addressed problem, the total states can be computed  $ |S| = \{𝑖𝑡𝑒𝑚𝑠_{𝑔𝑟𝑖𝑑}*move_{action}  \}$  where the items on the grid is calculated as the exponential relation of colored items number to grid size, and the move actions represent all the possible robot movements {store: blue/red/white, restore: blue/red/white }.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Transition probability matrix (TMP) in Markov processes stores the probability to transition from the current state to a next possible state after the agent has performed given action in a single time unit. The dimension if this matrix is determined by (|𝐴|,|𝑆|,|𝑆|).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reward matrix (R), is composed by the defined reward or symbolic benefit received performing and action 𝐴(𝑥,𝑦) for a given state. The shape of this matrix (|𝑆|,|𝐴|).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;transition-probability-matrix-tpm&#34;&gt;Transition Probability Matrix (TPM)&lt;/h3&gt;
&lt;p&gt;In the simple addressed problem, the warehouse grid is of size the 2x2. Therefore, so have 2·2=4 possible actions and 1536 different states. In order to compute the probability actions, is necessary to iterate through all possible actions as well as all generated states. Then assign the possibility of having a colored item or being empty derived from the training data frequencies.
It should also be taken into account if the warehouse grid is already full and if the linked operation is invalid. In that case, the transition probabilities are kept 0. To assert that the computation was correct, it was checked that all he probabilities for a particular state sum 1.&lt;/p&gt;
&lt;h3 id=&#34;regard-matrix&#34;&gt;Regard Matrix&lt;/h3&gt;
&lt;p&gt;The aim of the reinforcement learning algorithm is to maximize the obtained rewards. In this problem, the distance should influence the reward. Thus, the simple criteria to assign the negative distances values from the origin warehouse cell (1,1). At the same time, not from every state the robot can transition to other state. For the entries in the reward matrix, as the movement action would be invalid, a penalization of -10000 was taken as a reward.
With the necessary TMP and R, a MDP model is trained within the python-MDPtoolbox package. It contains the most common Reinforcement Learning training approaches, such as Value Iteration and Policy Iteration. These were the best models selected for the simplified problem. The discount factor and maximum iteration hyperparameters were set to 0.9 and 750 respectively.&lt;/p&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;MDP policy refers to a solution to which specifies an action for each state. Value iteration is defined as an algorithm that gives an optimal policy for a MDP, i.e., The ideal MDP solution. In the training run, for policy iteration 6 iterations were performed whereas for value iteration 35 were performed.
To evaluate the model, a test dataset containing 60 pair of movement actions and colored items was used. Although different MDP algorithms were tried, there was no change on the result. The movements distance need was 232 for both Value Iteration and Policy Iteration. That may be due to on the restricted problem, the simple algorithm is already learning the optimal policy for the given rewards.
In order to assess if the policy represents a better utility than random, a simple random walk approach through the grid was also implemented. In that the distance or movements performed by the robot are count. The random distance, although the random seed was fix, it rounds the range of 264.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Algorithm&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Move Distance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Value Iteration&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;232&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Policy Iteration&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;232&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Random Walk&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;264&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Example Post (Under construction)</title>
      <link>https://mariamonzon.com/post/getting-started/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/post/getting-started/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site&lt;/li&gt;
&lt;li&gt;The template can be modified and customised to suit your needs. It&amp;rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a &lt;strong&gt;no-code solution (write in Markdown and customize with YAML parameters)&lt;/strong&gt; and having &lt;strong&gt;flexibility to later add even deeper personalization with HTML and CSS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/wowchemy/wowchemy-hugo-modules/main/starters/academic/preview.png&#34; alt=&#34;The template is mobile first with a responsive design to ensure that your site looks stunning on every device.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;get-started&#34;&gt;Get Started&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;👉 &lt;a href=&#34;https://wowchemy.com/templates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Create a new site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;📚 &lt;a href=&#34;https://wowchemy.com/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Personalize your site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;💬 &lt;a href=&#34;https://discord.gg/z8wNYzb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chat with the &lt;strong&gt;Wowchemy community&lt;/strong&gt;&lt;/a&gt; or &lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Hugo community&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;🐦 Twitter: &lt;a href=&#34;https://twitter.com/wowchemy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@wowchemy&lt;/a&gt; &lt;a href=&#34;https://twitter.com/GeorgeCushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@GeorgeCushen&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%23MadeWithWowchemy&amp;amp;src=typed_query&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#MadeWithWowchemy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;💡 &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-themes/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Request a &lt;strong&gt;feature&lt;/strong&gt; or report a &lt;strong&gt;bug&lt;/strong&gt; for &lt;em&gt;Wowchemy&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;⬆️ &lt;strong&gt;Updating Wowchemy?&lt;/strong&gt; View the &lt;a href=&#34;https://wowchemy.com/docs/hugo-tutorials/update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Update Tutorial&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/updates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;crowd-funded-open-source-software&#34;&gt;Crowd-funded open-source software&lt;/h2&gt;
&lt;p&gt;To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.&lt;/p&gt;
&lt;h3 id=&#34;-click-here-to-become-a-sponsor-and-help-support-wowchemys-future-httpswowchemycomsponsor&#34;&gt;&lt;a href=&#34;https://wowchemy.com/sponsor/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;❤️ Click here to become a sponsor and help support Wowchemy&amp;rsquo;s future ❤️&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;As a token of appreciation for sponsoring, you can &lt;strong&gt;unlock &lt;a href=&#34;https://wowchemy.com/sponsor/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;these&lt;/a&gt; awesome rewards and extra features 🦄✨&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/wowchemy/hugo-academic-cli&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Academic CLI&lt;/a&gt;:&lt;/strong&gt; Automatically import publications from BibTeX&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inspiration&#34;&gt;Inspiration&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Check out the latest &lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt; of what you&amp;rsquo;ll get in less than 10 minutes, or &lt;a href=&#34;https://wowchemy.com/user-stories/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;view the &lt;strong&gt;showcase&lt;/strong&gt;&lt;/a&gt; of personal, project, and business sites.&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Page builder&lt;/strong&gt; - Create &lt;em&gt;anything&lt;/em&gt; with &lt;a href=&#34;https://wowchemy.com/docs/page-builder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;widgets&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/docs/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;elements&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit any type of content&lt;/strong&gt; - Blog posts, publications, talks, slides, projects, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create content&lt;/strong&gt; in &lt;a href=&#34;https://wowchemy.com/docs/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://wowchemy.com/docs/import/jupyter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Jupyter&lt;/strong&gt;&lt;/a&gt;, or &lt;a href=&#34;https://wowchemy.com/docs/install-locally/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt; - Fully customizable &lt;a href=&#34;https://wowchemy.com/docs/customization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;color&lt;/strong&gt; and &lt;strong&gt;font themes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Display Code and Math&lt;/strong&gt; - Code highlighting and &lt;a href=&#34;https://en.wikibooks.org/wiki/LaTeX/Mathematics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LaTeX math&lt;/a&gt; supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt; - &lt;a href=&#34;https://analytics.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt;, &lt;a href=&#34;https://disqus.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Disqus commenting&lt;/a&gt;, Maps, Contact Forms, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beautiful Site&lt;/strong&gt; - Simple and refreshing one page design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industry-Leading SEO&lt;/strong&gt; - Help get your website found on search engines and social media&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Galleries&lt;/strong&gt; - Display your images and videos with captions in a customizable gallery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Friendly&lt;/strong&gt; - Look amazing on every screen with a mobile friendly version of your site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language&lt;/strong&gt; - 34+ language packs including English, 中文, and Português&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-user&lt;/strong&gt; - Each author gets their own profile page&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Pack&lt;/strong&gt; - Assists with GDPR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand Out&lt;/strong&gt; - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-Click Deployment&lt;/strong&gt; - No servers. No databases. Only files.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;p&gt;Wowchemy and its templates come with &lt;strong&gt;automatic day (light) and night (dark) mode&lt;/strong&gt; built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the &lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demo&lt;/a&gt; to see it in action! Day/night mode can also be disabled by the site admin in &lt;code&gt;params.toml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/customization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Choose a stunning &lt;strong&gt;theme&lt;/strong&gt; and &lt;strong&gt;font&lt;/strong&gt;&lt;/a&gt; for your site. Themes are fully customizable.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2016-present &lt;a href=&#34;https://georgecushen.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;George Cushen&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Released under the &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-themes/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Assessment of cardiac valve motion on time-resolved MRI images using deep learning</title>
      <link>https://mariamonzon.com/project/valve-cardiac-motion-assesment/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/project/valve-cardiac-motion-assesment/</guid>
      <description>&lt;p&gt;The presented work aimed to develop a novel method for the task of valve motion
assessment for prospective slice tracking in CMR temporal image (CINE) acquisition. 
The objective this thesis &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; was to evaluate deep learning-based landmarks tracking methods to extract the
motion of the mitral valves throughout the cardiac cycle on time-resolved CMR four-chamber-
view (4CHV) images. The fully automated CNN based algorithm would improve the current slice following acquisition workflow, which needs from manual user intervention. 
The automatic imaging slice tracking will enable a more precise morphology and flow estimation, potentially
improving the diagnosis of diastolic dysfunction. During this project, an automatic deep learning algorithm using landmark detection was developed.&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Cardio-vascular diseases remain the leading cause of death worldwide [1]. In particular, diastolic dys-
function is an important cause of cardiac insufficiency, heart failure, potentially leading to premature
cardiovascular death if not diagnosed and treated for at an early disease stage. Left ventricular dias-
tolic dysfunction is estimated to affect from 27% to 43% in middle-aged adults [2] and its prevalence
increases with age.&lt;/p&gt;
&lt;p&gt;Diastolic dysfunction is defined as a malfunctioning filling of the heart during diastole [3]. Its diag-
nostic remains challenging as not only structural but functional abnormalities need to be evaluated [4].
Several non-invasive imaging techniques have been used for assessing diastolic dysfunction.
Typically, intra-cardiac haemodynamics are measured using Doppler echocardiography. The peak
velocities during early diastolic filling (E wave) and atrial contraction (A wave) are measured, and their
ratio is calculated [5]. The Doppler flow measures are influenced by multiple factors including age,
valve heart diseases, heart rate. Therefore, no single parameter is determinant enough to asses diastolic
dysfunction [6].&lt;/p&gt;
&lt;p&gt;Magnetic Resonance Imaging (MRI) is a noninvasive technique, well suited for disease diagnosis
and monitoring due to its high spatial and temporal resolution and excellent soft-tissue contrast. In-
deed, Cardiac Magnetic Resonance (CMR) provides information about heart structure and function,
particularly in soft tissue characterization without the need of any contrast agent. Furthermore, CMR
is useful for characterizing the anatomic valve morphology and cine images allows to visualize the valve
throughout the cardiac cycle [7, 8]. Indeed, CMR allows assessment of blood flow using phase-contrast
MRI (PC-MRI) [9].&lt;/p&gt;
&lt;p&gt;As the valves move during a cardiac cycle, the acquisition of a fixed 2D slice will not allow the
accurate time-resolved visualization of the valve and quantification of the blood flow: The valve moves
into and out of the MRI slice so cannot be seen on each cardiac phase, therefore, the quantified flow
through the valve is not correct. Despite the importance of correct valve flow assessment, time resolved
CMR is yet usually performed at fixed slice positions throughout the cardiac cycle Kozerke et al. [10]
introduced the prospective slice tracking concept. If the valve motion is known prior to the examina-
tion, the slice position can be updated for each cardiac phase. As this approach however requires a
dedicated pre-scan including an image-based algorithm to quantify the valve motion, very few related
work can be found applying this technique in a clinical setting.&lt;/p&gt;
&lt;p&gt;The recent development of deep learning has led to significant improvements in medical image
analysis [11]. The prominent algorithms for feature tracking include deep learning systems based on
convolutional neural network architectures. Specifically, for valve-tracking, a deep learning feature
tracking algorithm for automatic slice-tracking can be developed, overcoming many limitations of the
current valve-tracking techniques. The imaging slice tracking will enable a more precise morphology
and flow estimation, potentially improving the diagnosis of diastolic dysfunction.&lt;/p&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;The dataset is composed of Cardiac Magnetic Resonance Imaging 4CHV CINE series from 87
patients extracted from the Cardiac Atlas Project [Fon11]. The imaging protocol included CINE
images acquired in long-axis planes, but for our work we only selected the 4CHV long axis view.
The sequence of 4CHV CINE CMR are acquired on multiple 1.5T MRI scanners from
different vendors (Philips, Siemens, GE). The mean pixel spacing of all selected datasets
is 1.49 + 0.28 mm/pixel. Additionally the annotated dataset contains anatomical landmarks, i.e.
the mitral valve position at each temporal aframe nnotated by an experienced analyst. 
As a preprocessing step, the CINE series were interpolated into a fixed and temporal resolution and
horizontally flipped to have all the images oriented with the apex of the heart upwards. Finally,the
input images intensity were normalized to 0-1 range values. After cleaning the data, the dataset
contain 87 series that are split into 70% for training and the rest 30% equally between test and
validation. To increase the variability of the dataset, online data augmentation was performed
when training the network in forms of shift, center cropping, rotation, Guassian noise addition,
contrast enhancement and Gaussian blurring.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/valve-cardiac-motion-assesment/data-exampleslice.png&#34; alt=&#34;data-visualization&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;system-overview&#34;&gt;System Overview&lt;/h2&gt;
&lt;p&gt;The motion assesment system is composed of four main stages. The preprocessed CINE dataset are forwarded as input to a convolutional based neural network (CNN).
The final proposed system comprises two chained CNNs based on heatmap regression approach: Localization Network + Landmark Detection Network. The task of the localization CNN model is to detect the landmarks only in the first temporal frame of the full 4CHV CINE series. The complete system is designed to regress the mitral valve-annulus landmarks for each time-frame points. 
Finally, the valvular plane motion can be derived from the two predicted landmark distance. The predicted temporal coordinates are translated into mm-space.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/valve-cardiac-motion-assesment/system-overview.png&#34; alt=&#34;results-visualization&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Evaluated heatmap based regression approach for landmark detection yields to a superior performance than direct coordinate regression.Especially, the motion is better modeled by the two-stages architectures than with 3-D instead
of single network. The proposed system showed an accurate match between expert-annotated
and automatically detected landmarks for each time-frame. An example results of the model is shown in the below figure:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/valve-cardiac-motion-assesment/results-motion-curves.png&#34; alt=&#34;results-visualization&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

The complete system has an accuracy of 1.66+- 0.75 mm. The roughness metric has a similar value as the ground truth annotations R = 0.085 +- 0.045 and the total slope variation value is TVslope = 0.53 +- 0.28.&lt;/p&gt;
&lt;p&gt;Our proposed method allows landmark localization with sub-pixel accuracy. Experiments
show that our approach is able to correctly locate the landmark which can assess the prospective
slice tracking acquisition method. The results of this work can be summarized as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CNNs can be used to develop a time-resolved landmark tracking application without the
need of any user interaction.&lt;/li&gt;
&lt;li&gt;Heart landmark detection with the deep learning results can be improved by means of regressing heatmaps.&lt;/li&gt;
&lt;li&gt;The proposed two-stages method further allows a more accurate localization.&lt;/li&gt;
&lt;li&gt;A post-processing step with the non-linear least square fitting is able to refine the landmark
location. The additional step outputs subpixel maxima and therefore predicts a smooth
motion.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conclusion-and-outlook&#34;&gt;Conclusion and Outlook&lt;/h2&gt;
&lt;p&gt;The proposed system enables the valve tracking detection over time and therefore smooth valve motion assessment. 
Future work would focus on extension on the algorithm to 2CHV valve landmark detection. Evaluation with more test data
and further refinement of the network. An integration of a single CNN could be convenient for faster inference time. Finally, when the validity of the method is proved , the scanner integration would be the next step.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] World Health Organization, World Health Statistics 2019: Monitoring Health for the SDGs. Geneva, Switzerland:
World Health Organization, 2019.&lt;/p&gt;
&lt;p&gt;[2] M. Nayor, L. L. Cooper, D. M. Enserro, V. Xanthakis, M. G. Larson, E. J. Benjamin, J. Aragam, G. F. Mitchell, and
R. S. Vasan, “Left ventricular diastolic dysfunction in the community: Impact of diagnostic criteria on the burden,
correlates, and prognosis,” Journal of the American Heart Association, vol. 7, no. 11, 2018.&lt;/p&gt;
&lt;p&gt;[3] A. Kossaify and M. Nasr, “Diastolic dysfunction and the new recommendations for echocardiographic assessment
of left ventricular diastolic function: Summary of guidelines and novelties in diagnosis and grading,” Journal of
Diagnostic Medical Sonography, vol. 35, no. 4, pp. 317–325, 2019.&lt;/p&gt;
&lt;p&gt;[4] C. Gutierrez and D. G. Blanchard, “Diastolic heart failure: Challenges of diagnosis and treatment,” American Family
Physician, vol. 69, no. 11, pp. 2609–2616, 2004.&lt;/p&gt;
&lt;p&gt;[5] C. Dugo, M. Rigolli, A. Rossi, and G. A. Whalley, “Assessment and impact of diastolic function by echocardiography
in elderly patients.,” Journal of geriatric cardiology : JGC, vol. 13, no. 3, pp. 252–25260, 2016.&lt;/p&gt;
&lt;p&gt;[6] S. F. Nagueh, O. A. Smiseth, C. P. Appleton, I. Byrd, Benjamin F., H. Dokainish, T. Edvardsen, F. A. Flachskampf,
T. C. Gillebert, A. L. Klein, P. Lancellotti, P. Marino, J. K. Oh, B. Alexandru Popescu, and Waggoner, “Recom-
mendations for the Evaluation of Left Ventricular Diastolic Function by Echocardiography: An Update from the
American Society of Echocardiography and the European Association of Cardiovascular Imaging,” European Heart
Journal - Cardiovascular Imaging, vol. 17, pp. 1321–1360, 07 2016.&lt;/p&gt;
&lt;p&gt;[7] S. Shah, E. D. Chryssos, and H. Parker, “Magnetic resonance imaging: a wealth of cardiovascular information.,” The
Ochsner journal, vol. 9, no. 4, pp. 266–77, 2009.&lt;/p&gt;
&lt;p&gt;[8] K. Maganti, V. H. Rigolin, M. E. Sarano, and R. O. Bonow, “Valvular heart disease: diagnosis and management.,”
Mayo Clinic proceedings, vol. 85, pp. 483–500, may 2010.&lt;/p&gt;
&lt;p&gt;[9] P. Waheed, A. K. Naveed, and F. Farooq, “Cardiovascular magnetic resonance physics for clinicians: part I,” Journal
of the College of Physicians and Surgeons Pakistan, vol. 19, no. 4, pp. 207–210, 2009.&lt;/p&gt;
&lt;p&gt;[10] S. Kozerke, J. Schwitter, E. M. Pedersen, and P. Boesiger, “Aortic and mitral regurgitation: Quantification using
moving slice velocity mapping,” Journal of Magnetic Resonance Imaging, vol. 14, no. 2, pp. 106–112, 2001.&lt;/p&gt;
&lt;p&gt;[11] A. Maier, C. Syben, T. Lasser, and C. Riess, “A gentle introduction to deep learning in medical image processing,”
Zeitschrift f ̈ur Medizinische Physik, vol. 29, no. 2, pp. 86 – 101, 2019.&lt;/p&gt;
&lt;p&gt;[12] S. S. Yoon, E. Hoppe, M. Schmidt, C. Forman, P. Sharma, C. Tilmanns, A. Maier, and J. Wetzl, “Automatic Cardiac
Resting Phase Detection for Static Cardiac Imaging Using Deep Neural Networks,” in Proceedings of the Joint
Annual Meeting ISMRM-ESMRMB (27th Annual Meeting &amp;amp; Exhibition) (I. S. for Magnetic Resonance in Medicine,
ed.), 2019.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;All the algorithims will be developed in python using the Pytorch deep learning framework. The data
for training, validation and testing will be provided by Siemens Healthineers.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Myocardial Pathology Segmentation Combining Multi-Sequence Cardiac Magnetic Resonance Images</title>
      <link>https://mariamonzon.com/project/myocardial-scar-segmentation/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/project/myocardial-scar-segmentation/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Accurate segmentation of myocardial pathological tissue from cardiac magnetic resonance images (CMR), such as scar tissue and edema, f is crutial to the assessment of the severity of myocardial infarction (MI).
CMRis the gold standar to provide anatomical and functional information of heart. Specifically, late gadolinium enhancement (LGE) CMR sequence which is used to diagnosis MI, the T2-weighted CMR which resembles ischemic regions, and the balanced- Steady State Free Precession (bSSFP) cine sequence which captures cardiac motions . Combining these multi-sequence CMR data could provide reliable information regarding to the pathological as well as morphological information of the myocardium &lt;a href=&#34;http://www.sdspeople.fudan.edu.cn/zhuangxiahai/0/myops20/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;The input dataset  contains 45 cases of multi-sequence CMR from Myops2020 challenge &lt;a href=&#34;http://www.sdspeople.fudan.edu.cn/zhuangxiahai/0/myops20/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;









. Each case refers to a patient with three sequence CMR, i.e., LGE, T2 and bSSFP CMR. All these clinical data have got institutional ethic approval and have been anonymized.
The “masks” folder contains 45 cases of multi-sequence CMR, where each mask represents the segmentation map of the the corresponding CMR Slice image:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Label 1: normal myocardium&lt;/li&gt;
&lt;li&gt;Label 2: edema&lt;/li&gt;
&lt;li&gt;Label 3: scar&lt;/li&gt;
&lt;li&gt;Label 0: Background&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/myops-scar-segmentation/example.png&#34; alt=&#34;example&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The proposed approach involves a two-staged network. First a heatmap-based regression architecture was train to detect a small ROI and locate the myocardium, based on , in order to reduce task complexity. Sequentially a a U-Net based neural network was train to perform  multi-modal pathological region segmentation.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/myops-scar-segmentation/108_T2_1.png&#34; alt=&#34;pathology-example&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Financial Transaction  Text Clasifier</title>
      <link>https://mariamonzon.com/project/text-classifier-bayes/</link>
      <pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/project/text-classifier-bayes/</guid>
      <description>&lt;p&gt;The aim of the project is to classify financial transactions, stored in a datasheet file, into one of seven categories listed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Income&lt;/li&gt;
&lt;li&gt;Private (cash, deposit, donation, presents)&lt;/li&gt;
&lt;li&gt;Living (rent, additional flat expenses, &amp;hellip;)&lt;/li&gt;
&lt;li&gt;Standard of living (food, health, children, &amp;hellip;)&lt;/li&gt;
&lt;li&gt;Finance (credit, bank costs, insurances, savings)&lt;/li&gt;
&lt;li&gt;Traffic (public transport, gas stations, bike, car &amp;hellip;)&lt;/li&gt;
&lt;li&gt;Leisure (hobby, sport, vacation, shopping, &amp;hellip;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The selected approach is to train a Multinomial Naive Bayes classifier, fitted with the transaction word counts and class categories. Naive Bayes is a statistical classification technique based on Bayes probability theorem, considered as one most basic supervised learning algorithm. Naive Bayes classifier assumes that the features in a class are independent of other features.
The followed approach to implement the classifier can be is based on the standard steps of Machine Learning (ML) algorithms: data exploration and preprocessing, feature selection and transformation, classifier model definition and training. The final phase of the assignment is dedicated for results visualization and evaluation.&lt;/p&gt;
&lt;h2 id=&#34;data-exploration-and-preprocessing&#34;&gt;Data exploration and preprocessing&lt;/h2&gt;
&lt;p&gt;The essential first step is to import the dataset files into the python program, loaded as a pandas DataFrame data structure. The next step performed was a data quality assessment by printing the header columns, missing values datatypes and a short overview of the data samples. Furthermore, the unique values of each field as well as the label frequencies are printed to determine if the dataset suffers from class imbalance.
As a quality assessment result, data cleaning procedures were performed. The missing values were substituted the missing values with 0, to minimize the effect of these on the accuracy of the model. For the text fields, the data was standardized by removing capital, special and/or punctuation characters, German stopwords and 2 or a single character-words. Finally, the sentences were splitted into single words (tokenize).
After the data cleaning and preprocessing, to check if the process was done successfully, a final data inspection and a summary of the dataset by class was depicted. A secondary reason for the data inspection was to have an insight of the feature’s distributions and outlier identification.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/text-classifier-bayes/1-HistogramTransactionClass.png&#34; alt=&#34;transaction-visualization&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;feature-transformation&#34;&gt;Feature transformation&lt;/h2&gt;
&lt;p&gt;Feature transformation refers to translating the data into an appropriate format allowing the ML model to learn from the data. For instance, categorical data need to be converted to numerical data for the Naïve Bayes classifier. The categorical labels were converted to values ranged between 0-5.
A common step in ML is also feature selection, i.e selection of the features in dataset hypothesized to be the most descriptive. Therefore, all the numeric values were scaled between the range 0-1 to reduce the variance. The string features were vectorized and concatenated to create the feature matrix. The final feature dimension was 502. As an optional step, a model selector of the best K features was also implemented for feature dimensionality reduction.&lt;/p&gt;
&lt;h2 id=&#34;model-training&#34;&gt;Model Training&lt;/h2&gt;
&lt;p&gt;The selected classifier model is a Multinomial Naïve bayes classifier implemented in the sklearn library. As only few data samples were available in the dataset, a cross validation scheme has been used for training and evaluation. The data was splitted in  k=10 folds and with stratified sampling to mitigate the class imbalance. Note that even the predict phase was done in after fitting the classifier on the KFold iteration, but this does not interfere in the training process.&lt;/p&gt;
&lt;h2 id=&#34;evaluation-and-result-visualization&#34;&gt;Evaluation and Result visualization&lt;/h2&gt;
&lt;p&gt;As stated previously, the data samples present in the datasheet are not many to test the robustness of the classifier a cross validation scheme has been used for evaluation. For each class, the predicted probabilities are plotted in the next figure. The probabilities for each sample are close to 1, which resembles the a extreme probability assignament of Naïve Bayes Classifier.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/text-classifier-bayes/2-predicted-probabilities.png&#34; alt=&#34;probabilites&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In order to quantified the performance of the classifier, many quantitative metrics were computed. 
The most common evaluation method in classification is the so-called confusion matrix. The Matrix is represented for a binary classification although it can be extended to multiclass problem. The matrix represents the relation between correctly classified, i.e. true positives (TP)  and True Negatives (TN), and wrongly predicted samples , i.e. false negatives (FN) and false positives (FP)  for each class. The matrix can be displayed with absolute frequency values or normalized by each class total number of elements. The confusion matrix for the assignment are shown below:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/text-classifier-bayes/2-confussion-matrix.png&#34; alt=&#34;results&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;From the confusion matrix to assess the the quality of the predictions further evaluation metrics can be derived, a ranged from 0 to 1 (represents the best possible score):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Accuracy: metric of all the correctly classified samples over the total number of samples that asses the overall performance of the model 

$$ Accuracy = \frac{TP+TN}{TTP+TN+FP+FN}$$ 
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Precision:  proportion of correctly predicted samples (TP) to the total predicted samples as positive, therefore assess the correct prediction capability for each class.

$$Precision= \frac{TP}{TP+FP}$$
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recall: ratio of correctly predicted samples (TP) and actual total samples of such class, therefore an assessment metric of the ability to classify all correct instances per class.

$$ Recall = \frac{TP}{TP+FN}$$ 
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;F1-score: is the harmonic mean between precision (P) and recall (R) and asses the incorrectly predicted samples, especially in class imbalanced problems 

$$Precision= \frac{2 \cdot P \cdot R}{P+R}$$
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The model achieves a total weighted accuracy of 90% and an averaged F1-score of 90%.  The detailed evaluation metrics for each class are summarize in the following table:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Label&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Total  Samples&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Precision&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Recall&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;F1-score&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Finance&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;33&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.88&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.94&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Income&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;17&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.00&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Leisure&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;65&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.89&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.95&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.92&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Living&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;26&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.91&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.81&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.86&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Private&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;21&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.77&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.95&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.85&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Standard of living&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;47&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.91&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.85&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.88&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For further visualization of the results, two additional graphic representations are shown in Figure 3. A Receiver Operator Characteristic (ROC) curve is a representation to assess the performance of binary classifiers, but it can be adapted for a multiclass classification by computing a curve for each class. The curve was computed following one-vs-all approach so that each class considers that class label as a true and all others as negative. From this curve, a further metric is usually derived as a summary of ROC, the area under the curve (AUC).  When AUC value is 1, it means that the classifier is able to classify all the classes, on the contrary, when the values is 0.5, the classifier only can classify a random label or a constant value. As depicted in the evaluation ROC, for all classes the AUC measures are over 0.88. Furthermore, the Jaccard similarity is a similarity metric for samples of two class sets to determine which samples  are shared and which are distinct. It is defined as the number of the label intersection divided by the union of two label sets.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/text-classifier-bayes/metrics-roc-jaccard.png&#34; alt=&#34;metrics&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adaptive Frame Rate for Egocentric Video</title>
      <link>https://mariamonzon.com/project/adaptive-frame-sampling/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/project/adaptive-frame-sampling/</guid>
      <description>&lt;p&gt;The goal of the project seminar is to implement an adaptive sampling strategy to dynamically
tune the sampling rate of a wearable egocentric camera. The sampling rate will be tuned
based on context measure, i.e., a measure of motion, extracted from the frames.&lt;/p&gt;
&lt;p&gt;The increasing development new media and data acquisition techniques have lead to new
innovative video recording set ups. A clear example of that is the egocentric video, where
a camera on head or on the chest approximates the visual field of the camera wearer.
This new camera setting offers a valuable perspective to understand user&amp;rsquo;s activities and
their context. In this case, the wearable head-mounted egocentric camera set up pursued
a dietary event spotting in a free living condition.&lt;/p&gt;
&lt;p&gt;A main feature of free-living data is a long recording duration. However, this kind of
camera often acquire irrelevant data for the analysis task.
In this work we introduce an adaptive sampling strategy to dynamically tune the sam-
pling rate of a wearable egocentric camera. The adaptive sampling rate is based on a
context measure. It is a motion measure, indicative if the recorded activity might be of
our interest.&lt;/p&gt;
&lt;p&gt;To evaluate the sampling strategy we implemented a computational simulation of resource consumption in terms of energy consumption and memory storage. For this purpose, I designed an analytical energy and memory model, a
nd base our analysis on data extracted from datasheets. Our energy and memory model considers
both sensing components, i.e., the cmos sensor, and processing, i.e., the microcontroller
to process the deep neural network.&lt;/p&gt;
&lt;p&gt;The clear advantage of an adaptive frame rate is saving energy as the camera will not be
acquiring data continuously. 
Benefits from our method will be longer recording sessions, smaller battery employment or smaller
storage space usage. This would help to overcome the limitations of the video recording,
in terms of power consumption, while maintaining acceptable performance. 
Furthermore, less data acquisition will also mean an energy saving in processing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://mariamonzon.com/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://wowchemy.com/docs/content/slides/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://revealjs.com/pdf-export/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;porridge &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; porridge &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Eating...&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{{% speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;-&lt;/span&gt; Only the speaker can read these notes
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;-&lt;/span&gt; Press &lt;span style=&#34;color:#e6db74&#34;&gt;`S`&lt;/span&gt; key to view
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  {{% /speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{{&amp;lt; &lt;span style=&#34;color:#f92672&#34;&gt;slide&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;background-image&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/media/boards.jpg&amp;#34;&lt;/span&gt; &amp;gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{{&amp;lt; &lt;span style=&#34;color:#f92672&#34;&gt;slide&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;background-color&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;#0000FF&amp;#34;&lt;/span&gt; &amp;gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{{&amp;lt; &lt;span style=&#34;color:#f92672&#34;&gt;slide&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;class&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;my-style&amp;#34;&lt;/span&gt; &amp;gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-css&#34; data-lang=&#34;css&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;reveal&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;section&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;h1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;reveal&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;section&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;h2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;reveal&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;section&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;h3&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;color&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;navy&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://discord.gg/z8wNYzb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/content/slides/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Publications</title>
      <link>https://mariamonzon.com/portfolio/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/portfolio/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Publications</title>
      <link>https://mariamonzon.com/publications/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/publications/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Resume Page</title>
      <link>https://mariamonzon.com/cv/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/cv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Resume Page</title>
      <link>https://mariamonzon.com/education/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/education/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ECG-based algorithm for Annotation of Resuscitation Episode</title>
      <link>https://mariamonzon.com/project/ecg-annotation-svm-classifier/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/project/ecg-annotation-svm-classifier/</guid>
      <description>&lt;p&gt;Out-of hospital cardiac arrest (OHCA) is one of the major causes of death in developed countries. Resuscitation guidelines recommend different treatments depending on the heart rhythm of the patient. The objective of this work is to develop a machine learning algorithm based on the ECG signal to automatically label heart rhythms in resuscitation episodes, a key tool for the retrospectively evaluation and improvement of the quality treatment. This work would help to systematise the annotation of databases since manual annotation of rhythms is a time-consuming task which can be an obstacle for handling large data sets.&lt;/p&gt;
&lt;p&gt;The starting point of this project was a database composed of 1631 intervals of 3 seconds taken from a larger database containing OHCA 298 episodes. To review the ECG segments, a graphical interface (GUI) was developed which allows the display of the ECGs classified by the type of arrhythmia: 
asystole (AS), ventricular tachycardia (VT) that degenerates into ventricular fibrillation (VF), pulseless electrical activity (PEA) pulse generating rhythms (PR).&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/egc-svm-annotation/GUI-visualization.png&#34; alt=&#34;GUI-example-visualization&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The database has been processed using a machine learning algorithm and the results obtained using cross-validation. Two classifiers have been developed selecting five features of the ECG, first to identify AS, and then to discriminate organised rhythms (PR and PEA) from ventricular arrhythmias (VT and VF).&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/egc-svm-annotation/ECG-classification-Example.png&#34; alt=&#34;ECG-Visualization&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;These algorithms have been combined to create a three class rhythm classification algorithm. The total accuracy of the final algorithm was 90.9%. A precise algorithm was obtained for the classification of OHCA rhythm into: AS, organised,  and  shockable  rhythms.  This  algorithm  can  be  implemented  to  analyse resuscitation episodes using 3 seconds ECG segments, and could be integrated into new methods for retrospective analysis of OHCA.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/egc-svm-annotation/SVM-classification-results.png&#34; alt=&#34;ECG-Visualization&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The results show that it is possible to automatically  interpret  resuscitation  cardiac  rhythm. These types of algorithms can be very useful since they allow an efficient rhythm classification with a minimum level of expert clinician supervision&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://mariamonzon.com/admin/index/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/admin/index/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
