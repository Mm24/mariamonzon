<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Android App | Maria Monzon</title>
    <link>https://mariamonzon.com/tag/android-app/</link>
      <atom:link href="https://mariamonzon.com/tag/android-app/index.xml" rel="self" type="application/rss+xml" />
    <description>Android App</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 15 Sep 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://mariamonzon.com/media/icon_huaad826f52d8c51d03abf0418cbb027d5_448596_512x512_fill_lanczos_center_3.png</url>
      <title>Android App</title>
      <link>https://mariamonzon.com/tag/android-app/</link>
    </image>
    
    <item>
      <title>Biomarkers Voice Clasifier App</title>
      <link>https://mariamonzon.com/project/voice-classifier-app/</link>
      <pubDate>Wed, 15 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/project/voice-classifier-app/</guid>
      <description>&lt;p&gt;This project aim is to embeed a sound classification moden into simple android app for learning purposes. The model that classify 2-second audio samples is a small convolutional neural network.
Sound classification is a machine learning task where you input some sound to a machine learning model to categorize it into predefined categories such as singing and speech. There are already many applications of sound classification.&lt;/p&gt;
&lt;h2 id=&#34;__dataset__&#34;&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The first step to develop the model is to find a suitable dataset. For the siging database, the&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://smcnus.comp.nus.edu.sg/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NUS-48E Sung and Spoken Corpus&lt;/a&gt; developed at Sound and Music Computing Laboratory at National University of Singapore was used.
The corpus is a 169-min collection of audio recordings of the sung and spoken lyrics of 48 (20 unique) English songs by 12 subjects and a complete set of transcriptions and duration annotations at the phone-level for all recordings of sung lyrics, comprising 25,474 phone instances.&lt;/p&gt;
&lt;p&gt;The  training datasetwas further pre-process by convert them to the WAV format and splitting them in 2 seconds window.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-comparison-of-spectrogram-of-a-spoken-lyrics-and-its-corresponding-singing-signal-for-the-sentence-i-believe-that-the-heart-does-go-on-adapted-from-1httpswwwsemanticscholarorgpapernus-hlt-spoken-lyrics-and-singing-sls-corpus-gao-sismanfdc6c08d687114efe37bf11175542f43890e62fa&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/voice-classifier-app/dataset.png&#34; alt=&#34;dataset&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Comparison of spectrogram of a spoken lyrics and its corresponding singing signal for the sentence &amp;lsquo;I believe that the heart does go on&amp;rsquo; Adapted from &lt;a href=&#34;https://www.semanticscholar.org/paper/NUS-HLT-Spoken-Lyrics-and-Singing-%28SLS%29-Corpus-Gao-Sisman/fdc6c08d687114efe37bf11175542f43890e62fa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;For the background samples, the dataset was complemented with the &lt;a href=&#34;https://github.com/karolpiczak/ESC-50&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ESC-50 dataset&lt;/a&gt;. ESC-50 consist of a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification. The dataset consists of 5-second-long recordings organized into 50 semantical classes (with 40 examples per class) loosely arranged into 5 major categories:&lt;/p&gt;
&lt;h2 id=&#34;__model__&#34;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The proof-of-concept model is just a basic 1D CNN model.
The model receives a 1D time representation of sound. It first processes the time eries with successive layers of 2D convolution (Conv1D) bi-layers with ReLU activations.&lt;/p&gt;
&lt;p&gt;The model ends in a number of dense (fully-connected) layers, which are interleaved with dropout layers for the purpose of reducing overfitting during training. The final output of the model is an array of probability scores, one for each class of sound the model is trained to recognize. The model was trained on Google Collab to take advantage of free GPU. For the integration into the app, the trained model was deployed with TensorFlow Lite.&lt;/p&gt;
&lt;h2 id=&#34;__app-deployment__&#34;&gt;&lt;strong&gt;App Deployment&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Android sample app was the starting point to design the custom app. It enables to acquire the microphone data for over 2 seconds when tiping the record button.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/voice-classifier-app/app.png&#34; alt=&#34;app-visualization&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;__references__&#34;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.semanticscholar.org/paper/NUS-HLT-Spoken-Lyrics-and-Singing-%28SLS%29-Corpus-Gao-Sisman/fdc6c08d687114efe37bf11175542f43890e62fa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt; Gao, X., Sisman, B., Das, R., &amp;amp; Vijayan, K. (2018). NUS-HLT Spoken Lyrics and Singing (SLS) Corpus. 2018 International Conference on Orange Technologies (ICOT), 1-6.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
