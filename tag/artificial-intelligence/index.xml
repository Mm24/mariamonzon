<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Artificial Intelligence | Maria Monzon</title>
    <link>https://mariamonzon.com/tag/artificial-intelligence/</link>
      <atom:link href="https://mariamonzon.com/tag/artificial-intelligence/index.xml" rel="self" type="application/rss+xml" />
    <description>Artificial Intelligence</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 15 Sep 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://mariamonzon.com/media/icon_huaad826f52d8c51d03abf0418cbb027d5_448596_512x512_fill_lanczos_center_3.png</url>
      <title>Artificial Intelligence</title>
      <link>https://mariamonzon.com/tag/artificial-intelligence/</link>
    </image>
    
    <item>
      <title>Biomarkers Voice Clasifier App</title>
      <link>https://mariamonzon.com/project/voice-classifier-app/</link>
      <pubDate>Wed, 15 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/project/voice-classifier-app/</guid>
      <description>&lt;p&gt;This project aim is to embeed a sound classification moden into simple android app for learning purposes. The model that classify 2-second audio samples is a small convolutional neural network. 
Sound classification is a machine learning task where you input some sound to a machine learning model to categorize it into predefined categories such as singing and speech. There are already many applications of sound classification.&lt;/p&gt;
&lt;h2 id=&#34;__dataset__&#34;&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The first step to develop the model is to find a suitable dataset. For the siging database, the&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://smcnus.comp.nus.edu.sg/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NUS-48E Sung and Spoken Corpus&lt;/a&gt; developed at Sound and Music Computing Laboratory at National University of Singapore was used. 
The corpus is a 169-min collection of audio recordings of the sung and spoken lyrics of 48 (20 unique) English songs by 12 subjects and a complete set of transcriptions and duration annotations at the phone-level for all recordings of sung lyrics, comprising 25,474 phone instances.&lt;/p&gt;
&lt;p&gt;The  training datasetwas further pre-process by convert them to the WAV format and splitting them in 2 seconds window.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-comparison-of-spectrogram-of-a-spoken-lyrics-and-its-corresponding-singing-signal-for-the-sentence-i-believe-that-the-heart-does-go-on-adapted-from-1httpswwwsemanticscholarorgpapernus-hlt-spoken-lyrics-and-singing-sls-corpus-gao-sismanfdc6c08d687114efe37bf11175542f43890e62fa&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/voice-classifier-app/dataset.png&#34; alt=&#34;dataset&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Comparison of spectrogram of a spoken lyrics and its corresponding singing signal for the sentence &amp;lsquo;I believe that the heart does go on&amp;rsquo; Adapted from &lt;a href=&#34;https://www.semanticscholar.org/paper/NUS-HLT-Spoken-Lyrics-and-Singing-%28SLS%29-Corpus-Gao-Sisman/fdc6c08d687114efe37bf11175542f43890e62fa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;For the background samples, the dataset was complemented with the &lt;a href=&#34;https://github.com/karolpiczak/ESC-50&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ESC-50 dataset&lt;/a&gt;. ESC-50 consist of a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification. The dataset consists of 5-second-long recordings organized into 50 semantical classes (with 40 examples per class) loosely arranged into 5 major categories:&lt;/p&gt;
&lt;h2 id=&#34;__model__&#34;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The proof-of-concept model is just a basic 1D CNN model.
The model receives a 1D time representation of sound. It first processes the time eries with successive layers of 2D convolution (Conv1D) bi-layers with ReLU activations.&lt;/p&gt;
&lt;p&gt;The model ends in a number of dense (fully-connected) layers, which are interleaved with dropout layers for the purpose of reducing overfitting during training. The final output of the model is an array of probability scores, one for each class of sound the model is trained to recognize. The model was trained on Google Collab to take advantage of free GPU. For the integration into the app, the trained model was deployed with TensorFlow Lite.&lt;/p&gt;
&lt;h2 id=&#34;__app-deployment__&#34;&gt;&lt;strong&gt;App Deployment&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Android sample app was the starting point to design the custom app. It enables to acquire the microphone data for over 2 seconds when tiping the record button.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/voice-classifier-app/app.png&#34; alt=&#34;app-visualization&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;__references__&#34;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.semanticscholar.org/paper/NUS-HLT-Spoken-Lyrics-and-Singing-%28SLS%29-Corpus-Gao-Sisman/fdc6c08d687114efe37bf11175542f43890e62fa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt; Gao, X., Sisman, B., Das, R., &amp;amp; Vijayan, K. (2018). NUS-HLT Spoken Lyrics and Singing (SLS) Corpus. 2018 International Conference on Orange Technologies (ICOT), 1-6.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Warehouse storing route optimization with Reinforcement Learning</title>
      <link>https://mariamonzon.com/project/reinforcement-learning-warehouse/</link>
      <pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/project/reinforcement-learning-warehouse/</guid>
      <description>&lt;p&gt;The problem represents a storage decision process where outcome is under the control of a robot, i.e, a decision maker
agent, but also are partly random. Therefore, the problem can be well modeled as a discrete-time Markov Decision
Processes. The approach follow was:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implement a reinforcement-learning based algorithm&lt;/li&gt;
&lt;li&gt;The robot is the agent and decides where to place the next part&lt;/li&gt;
&lt;li&gt;Use the markov decision process toolbox for your solution&lt;/li&gt;
&lt;li&gt;Choose the best performing MDP&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;defitinions&#34;&gt;Defitinions&lt;/h2&gt;
&lt;p&gt;The basic concepts to understand the promblem are shortly introduced based on &lt;em&gt;Artificial Intelligence: A Modern Approach&lt;/em&gt; book:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reinforcement learning&lt;/strong&gt;: Type of dynamic programming that trains algorithms using
a system of reward and punishment. The agent learns without intervention from a human by maximizing its reward
and minimizing its penalty and updates itself continuously. The algorithm it automatically finds patterns
and relationships inside of that dataset. It requires realtime data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Agent&lt;/strong&gt;: A is the set of all possible moves the agent can make. An action is almost self-explanatory, but it should be noted that agents choose among a list of possible actions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Environment&lt;/strong&gt;: The world through which the agent moves. The environment takes the agent&amp;rsquo;s current state and action as input, and returns as output the agent&amp;rsquo;s reward and its next state.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;State&lt;/strong&gt;: The parameter values that describe the current cofiguration of the environment, which the agent uses to choose an action. A state is a concrete and immediate situation in which the agent finds itself; i.e. a specific place and moment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reward&lt;/strong&gt;: Feedback by which we effectively evaluate the agent&amp;rsquo;s action.
From any given state, an agent sends output in the form of actions to the environment, and the environment
returns the agent&amp;rsquo;s new state (which resulted from actingon the previous state) as well as rewards, if any.
Rewards
can be immediate or delayed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Markov decision process&lt;/strong&gt;: Markov decision processes (MDPS) is a model decision making in stochastic, sequential environments. The essence of the model is that a decision maker, or agent, inhabits an environment, which changes state randomly in response to action choices made by the agent. 
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/reinforcement-learning-warehouse/agent-environment.png&#34; alt=&#34;agent&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-introduction&#34;&gt;Example Introduction&lt;/h2&gt;
&lt;p&gt;A tiny warehouse with (2x2) storage capacity locations is simulated. The picking robot (agent) interacts with the
warehouse (environment) by store-restoring items in each warehouse cell or shelve. The dataset contains store and
restore action for red, blue and white colored items although an empty field is also possible.&lt;/p&gt;
&lt;p&gt;When the agent is placed on a field position (𝑥𝑑, 𝑦𝑑), it can either store or restore each of the color items.
There exists a total of six possible actions to change its environment. The robot can move in the (2x2) grid
environment and start moving at initial grid position (1,1) . The robot is constrained to always to move only
to one adjacent fields (not in diagonal).&lt;/p&gt;
&lt;p&gt;The distance the robot needs to move is derived based on current (𝑥𝑐 , 𝑦𝑐 ) and goal position (𝑥𝑑 , 𝑦𝑑). The
distance is calculated as the sum of the absolute differences of the layout position

$$ 𝑑 = |𝑥_𝑑 − 𝑥_𝑐 | + |𝑦_𝑑 − 𝑦_𝑑|$$ 
&lt;/p&gt;
&lt;p&gt;Therefore, the distance to the position can be assigned to the cost of the action or negative reward. The ideal
goal of the reinforcement learning approach would be to optimize the route picking storage strategy by
minimizing the distance with rewards the store/restore motion actions.&lt;/p&gt;
&lt;h2 id=&#34;methods&#34;&gt;Methods&lt;/h2&gt;
&lt;p&gt;The Markov Discrete Process (MDP) algorithm was implemented for modelling the problem. A MDP is
described by state transition probabilities. A general Reinforcement Learning and thus also MDP algorithm
is defined with a set of variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Actions (A): refers to the operations an agent can perform which direct modify the environment. In our
problem this are related to the effect on the warehouse grid cells, directly proportional to the warehouse size
(𝑋, 𝑌). $ A = \{ 𝐴_{(1,1)}, 𝐴_{(1,2)}, 𝐴_{(2,1)}, … , 𝐴_{(X,Y)}\}$  where  $ |𝐴| = |𝑋 \cdot 𝑌|$ .&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;States (S): represent all the possible configuration of environment, i.e, how the items are storage in the warehouse grid. In the addressed problem, the total states can be computed  $ |S| = \{𝑖𝑡𝑒𝑚𝑠_{𝑔𝑟𝑖𝑑}*move_{action}  \}$  where the items on the grid is calculated as the exponential relation of colored items number to grid size, and the move actions represent all the possible robot movements {store: blue/red/white, restore: blue/red/white }.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Transition probability matrix (TMP) in Markov processes stores the probability to transition from the current state to a next possible state after the agent has performed given action in a single time unit. The dimension if this matrix is determined by (|𝐴|,|𝑆|,|𝑆|).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reward matrix (R), is composed by the defined reward or symbolic benefit received performing and action 𝐴(𝑥,𝑦) for a given state. The shape of this matrix (|𝑆|,|𝐴|).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;transition-probability-matrix-tpm&#34;&gt;Transition Probability Matrix (TPM)&lt;/h3&gt;
&lt;p&gt;In the simple addressed problem, the warehouse grid is of size the 2x2. Therefore, so have 2·2=4 possible actions and 1536 different states. In order to compute the probability actions, is necessary to iterate through all possible actions as well as all generated states. Then assign the possibility of having a colored item or being empty derived from the training data frequencies.
It should also be taken into account if the warehouse grid is already full and if the linked operation is invalid. In that case, the transition probabilities are kept 0. To assert that the computation was correct, it was checked that all he probabilities for a particular state sum 1.&lt;/p&gt;
&lt;h3 id=&#34;regard-matrix&#34;&gt;Regard Matrix&lt;/h3&gt;
&lt;p&gt;The aim of the reinforcement learning algorithm is to maximize the obtained rewards. In this problem, the distance should influence the reward. Thus, the simple criteria to assign the negative distances values from the origin warehouse cell (1,1). At the same time, not from every state the robot can transition to other state. For the entries in the reward matrix, as the movement action would be invalid, a penalization of -10000 was taken as a reward.
With the necessary TMP and R, a MDP model is trained within the python-MDPtoolbox package. It contains the most common Reinforcement Learning training approaches, such as Value Iteration and Policy Iteration. These were the best models selected for the simplified problem. The discount factor and maximum iteration hyperparameters were set to 0.9 and 750 respectively.&lt;/p&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;MDP policy refers to a solution to which specifies an action for each state. Value iteration is defined as an algorithm that gives an optimal policy for a MDP, i.e., The ideal MDP solution. In the training run, for policy iteration 6 iterations were performed whereas for value iteration 35 were performed.
To evaluate the model, a test dataset containing 60 pair of movement actions and colored items was used. Although different MDP algorithms were tried, there was no change on the result. The movements distance need was 232 for both Value Iteration and Policy Iteration. That may be due to on the restricted problem, the simple algorithm is already learning the optimal policy for the given rewards.
In order to assess if the policy represents a better utility than random, a simple random walk approach through the grid was also implemented. In that the distance or movements performed by the robot are count. The random distance, although the random seed was fix, it rounds the range of 264.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Algorithm&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Move Distance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Value Iteration&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;232&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Policy Iteration&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;232&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Random Walk&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;264&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
  </channel>
</rss>
