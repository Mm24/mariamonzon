<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | Maria Monzon</title>
    <link>https://mariamonzon.com/tag/machine-learning/</link>
      <atom:link href="https://mariamonzon.com/tag/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 May 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://mariamonzon.com/media/icon_huaad826f52d8c51d03abf0418cbb027d5_448596_512x512_fill_lanczos_center_3.png</url>
      <title>Machine Learning</title>
      <link>https://mariamonzon.com/tag/machine-learning/</link>
    </image>
    
    <item>
      <title>Deep Learning based reach-and-grasp EEG decoder</title>
      <link>https://mariamonzon.com/project/eeg-grasp-decoder/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/project/eeg-grasp-decoder/</guid>
      <description>&lt;p&gt;Decoding three different executed reach-and-grasp actions utilizing their electroencephalogram (EEG) recording from different electrodes is of crutial significance for the rehabilitation of hand functions of patients with motor disorders &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/28853420/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;.
Despite the high freedom of the human hand movements, most actions of daily life can be executed incorporating only palmar, lateral and grasp.  Recent studies have already shown that neural correlates of natural reach-and-grasp actions can be identified in the EEG &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/28853420/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2020.00849/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Deep Learning has recently achieved promising results in the field of Computer Vision and Biomedical Engineering. Therefore, this work aims to study the possibility of develop Deep learning based decoders to classify grasp actions based on EEG signals.  We have also studied the possibility of developing intersubject classifiers and transfer learning between the different subject technologies. For this purpose, different neural network architectures have been tested, single trial vs crop trial performance has been evaluated as well as the different training techniques: within-subject and inter-subject training.&lt;/p&gt;
&lt;h3 id=&#34;1-eeg-introduction&#34;&gt;1-EEG Introduction&lt;/h3&gt;
&lt;p&gt;The EEG is a cost-effective, non-invasive technique to examine brain activity linked to multiple neurocognitive processes that underlie human behavior. It consists of placing electrodes on the head to monitor the electrical activity produced when neurons fire. The EEG records and measures electrical signals of the human brain from multiple cortical areas. Therefore, EEG monitoring allows to quantify different types of brain waves, also known as neural oscillations.
The standard pipeline followed to extract information is depicted in the next figure:&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-eeg-standard-decoding-pipeline-adapted-from-bitbrain-three-important-steps-when-processing-eeg-april-23-2020-httpswwwbitbraincomblogai-eeg-data-processing&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/eeg-grasp-decoder/methods-eeg-decoding.png&#34; alt=&#34;methods-eeg-decoding&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      EEG standard decoding pipeline. Adapted from 
Bitbrain. “Three important steps when processing EEG”, April 23, 2020, &lt;a href=&#34;https://www.bitbrain.com/blog/ai-eeg-data-processing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.bitbrain.com/blog/ai-eeg-data-processing&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;2-dataset&#34;&gt;2-Dataset&lt;/h3&gt;
&lt;p&gt;In a cue-guided experiment, 15 healthy individuals were asked to perform reach-and-grasp actions using daily life objects.  The dataset is publicly available at &lt;a href=&#34;http://bnci-horizon-2020.eu/database/data-sets&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BNCI Horizon 2020&lt;/a&gt;. 
The pre-recorded dataset contains 7 min runs, leading to 80 trials per condition (TPC) distributed over 4 runs / 20 trials for each reach-and-grasp condition and from a no-movement condition.
The 45 right handed participants performed two self-initiated reach-and-grasp (palmar and lateral grasp) movement conditions.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-reach-and-grasp-movement-decoding-from-eeg-with-gel-water-and-dry-electrodes-dataset-experimental-set-up-2httpswwwfrontiersinorgarticles103389fnins202000849full&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/eeg-grasp-decoder/grasp-dataset-definition.png&#34; alt=&#34;grasp-task&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Reach and Grasp movement decoding from EEG with gel, water and dry electrodes dataset experimental set-up &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2020.00849/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gel-based electrodes recordings. EEG was measured with 58 electrodes (frontal, central and parietal areas).&lt;/li&gt;
&lt;li&gt;Water-based electrodes recordings mobile and water-based electrodes EEG-Versatile™ system with 32 electrodes&lt;/li&gt;
&lt;li&gt;Dry-electrodes recordings measured using the dry-electrodes EEG-Hero™ headset.  EEG was measured with 11 electrodes over the sensorimotor cortex.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;data-proprocesssing&#34;&gt;Data Proprocesssing&lt;/h4&gt;
&lt;p&gt;The EEG data processsing was analogous to the one in &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2020.00849/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt;. All the modalities data were filtered with a zero-phase 4th order Butterworth filter with a cut-off frequency of 0.3 and resample to 128 Hz. 
We defined a window of interest for each movement trial of [-2 3] s with respect to the movement onset at second 0.
In addition, we also extracted 81 rest trials from inactivity periods with a duration of 5 seconds.&lt;/p&gt;
&lt;h3 id=&#34;3-methods&#34;&gt;3-Methods&lt;/h3&gt;
&lt;h4 id=&#34;vanilla-1d-network&#34;&gt;Vanilla 1D Network&lt;/h4&gt;
&lt;p&gt;We aimed to design a single convolutional neural network (CNN) architecture to accurately classify grasp actions from differente EEG decoding modalitites, while  being as compact and simple as possible. We try a simple vanilla 1D convolutional neural network based on 1D temporal convolutions in order to encapsulate EEG feature extraction methodologies used in traditional classiers &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2020.00849/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt;
















&lt;figure  id=&#34;figure-overview-of-1d-cnn-designed-architecture-it-contains-a-1d-convolution-block-layer-followed-by-a-temporal-pooling-and-convolution-kernel-to-extract-features-that-are-the-input-for-the-dense-layer&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/eeg-grasp-decoder/network-description.png&#34; alt=&#34;methods-eeg-network&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Overview of 1D CNN designed architecture. It contains a 1D convolution block layer followed by a temporal pooling and convolution kernel to extract features that are the input for the dense layer
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;eegnet-3httpsiopscienceioporgarticle1010881741-2552aace8c&#34;&gt;EEGNet &lt;a href=&#34;https://iopscience.iop.org/article/10.1088/1741-2552/aace8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[3]&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;EEGNet is a compact CNN designed for BCIs that can be trained with very limited data. The architecture has three convolution layers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a one-dimensional convolution analogous to temporal band-pass filtering&lt;/li&gt;
&lt;li&gt;a depthwise convolution to perform spatial filtering,&lt;/li&gt;
&lt;li&gt;a separable convolution to identify temporal patterns across the previous filters&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;htnet-architecture-4httpsdoiorg1010881741-2552abda0b&#34;&gt;HTNet architecture &lt;a href=&#34;https://doi.org/10.1088/1741-2552/abda0b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[4]&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;HTNet builds upon EEGNet &lt;a href=&#34;https://iopscience.iop.org/article/10.1088/1741-2552/aace8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[3]&lt;/a&gt;. The authors added a Hilbert transform layer after this initial temporal convolution to compute relevant spectral power features using a data-driven filter-Hilbert.  The temporal convolution and Hilbert transform layers generate data-driven spectral features that can then be projected from electrodes onto common regions of interest using a predefined weight matrix.&lt;/p&gt;
&lt;h4 id=&#34;training-strategies&#34;&gt;Training Strategies&lt;/h4&gt;
&lt;p&gt;Transfer learning techniques from the field of machine learning have been adopted also for EEG feature distribution for inter-subject variability. The common cross-validation strategy used in EEG decoding is known as &amp;ldquo;leave one-subject-out&amp;rdquo;. Given the N subjects, the training subset is fromed by N - 1, while the remaining subject is used for testing.
Classiffcation results are reported for differente training stratesgies: within-subject, inter-subject and with pretraining in another recording technology.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-test-strategies-for-the-evaluation-of-the-resutls&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/eeg-grasp-decoder/methods-training-valdation-strategy.png&#34; alt=&#34;methods-training&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Test strategies for the evaluation of the resutls
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;data-augmentation&#34;&gt;Data Augmentation&lt;/h4&gt;
&lt;p&gt;Data augmentation refers to techniques used to increase the amount of data by slightly modifying training data. 
Data augmentation is especially useful for EEG signals where the limitation of small-scale datasets greatly affects the performance of classifiers. Still due to the variability of EEG and time-series nature, it is challenging to augment the data in the feature space. Based on the findings of &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2020.00849/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt;, we implemented an easy on-the-fly data augmentation that consist on band filtering the training data. The aim is to enforce the network to learn different features at different frequency bands.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-overview-of-training-pipeline-including-the-data-augmentation-method&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/eeg-grasp-decoder/methods-split-bands.png&#34; alt=&#34;methods-augmentation&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Overview of training pipeline including the data augmentation method
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;4-results&#34;&gt;4-Results&lt;/h3&gt;
&lt;p&gt;In a single-trial multiclass based decoding approach, which incorporated both movement conditions and rest
can be successfully decodes using Deep learning based decoders. 
We performed a comparison on the decoding accuracy for single trial of 2 seconds on the state-of-the-art network architecture on the time of the study.  Table depicts the inter-participant classification results.
















&lt;figure  id=&#34;figure-decoding-accuracy-on-single-trial-classification&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/eeg-grasp-decoder/results-architecture.png&#34; alt=&#34;results-architecture&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Decoding accuracy on single trial classification
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The effect of cropping window duration was investigated. We can conlcude that longer signal windows show better performance.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-overview-of-training-pipeline-including-the-data-augmentation-method&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/eeg-grasp-decoder/results-cropping-window.png&#34; alt=&#34;results-window&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Overview of training pipeline including the data augmentation method
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The best results were achieved when training the model with a cropped window T =[0,1] with overlapping strides 250ms. The models were pretrained in another recording technology resampled to 128 Hz with split frequencies data augmentation strategy. On average, best classification performance could be reached 1s after the movement onset.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-single-trial-decoding-performance-of-the-dry--and-water-electrodes-recordings&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/eeg-grasp-decoder/results-accuracy-time.png&#34; alt=&#34;results-accuracy&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Single trial decoding performance of the dry- and water-electrodes recordings
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Despite the reduced number of channels of the dry electrodes recordings, the average performance was not decreased significantly as it can be seen in the above figure.&lt;/p&gt;
&lt;h4 id=&#34;5-conclusion&#34;&gt;5-Conclusion&lt;/h4&gt;
&lt;p&gt;This study confirmed that EEG based correlates of reach-and-grasp actions can be successfully identified  using Deep Leaning based decoders. We demonstrated that a simple, yet effective,  1D convolution CNN  can reach state-of-the-art neural decoders when and improve the results appliying the mmodes  to new participants, even when a different recording modality is used. Unfortunately, a direct comparison to other reach-and-grasp studies such as is difficult due significant differences in experimental setup and paradigm and hence cannot be made in a serious manner.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/28853420/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt; Schwarz A, Ofner P, Pereira J, Sburlea AI, Müller-Putz GR. Decoding natural reach-and-grasp actions from human EEG. J Neural Eng. 2018 Feb;15(1):016005. doi: 10.1088/1741-2552/aa8911. PMID: 28853420.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2020.00849/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt; Schwarz, A., Escolano, C., Montesano, L., &amp;amp; Müller-Putz, G. (2020). Analyzing and Decoding Natural Reach-and-Grasp Actions Using Gel, Water and Dry EEG Systems. Frontiers in Neuroscience, 14.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://iopscience.iop.org/article/10.1088/1741-2552/aace8c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[3]&lt;/a&gt; Lawhern V J, Solon A J, Waytowich N R, Gordon S M, Hung C P and Lance B J 2018 Eegnet: a compact convolutional neural network for EEG-based brain–computer interfaces J. Neural Eng. 15 056013&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.1088/1741-2552/abda0b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[4]&lt;/a&gt; Peterson, S. M., Steine-Hanson, Z., Davis, N., Rao, R. P. N., &amp;amp; Brunton, B. W. (2021). Generalized neural decoders for transfer learning across participants and recording modalities.
Journal of Neural Engineering. &lt;a href=&#34;https://doi.org/10.1088/1741-2552/abda0b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1088/1741-2552/abda0b&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fully automatic extraction of mitral valve annulus motion parameters on long axis CINE CMR using deep learning</title>
      <link>https://mariamonzon.com/project/cardiac-parameter-extraction/</link>
      <pubDate>Thu, 31 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/project/cardiac-parameter-extraction/</guid>
      <description>&lt;p&gt;Diastolic dysfunction is an important cause of cardiac insuf-
ficiency, defined as a malfunctioning filling of the heart during diastole.
The analysis of mitral valve motion is known to be relevant in the diagno-
sis of cardiac dysfunction. Cardiac motion parameters can be extracted
from Cardiac Magnetic Resonance (CMR) images. However, in clinical
setting valve motion modeling usually needs a manual intervention to lo-
calize the valvular plane. We propose two chained Convolutional Neural
Networks (CNN) for automatic tracking of mitral valve-annulus land-
marks on time-resolved 2-chamber and 4-chamber CMR images. 
The first CNN is trained to detect the region of interest and the second to
track the landmarks along the cardiac cycle. The presented deep learn-
ing system has high accuracy in terms of temporal landmark tracking
and motion assessment. Furthermore, we successfully extracted several
motion-related parameters thereby overcoming time-consuming annota-
tion and allowing statistical analysis over a large number of datasets.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adaptive Frame Rate for Egocentric Video</title>
      <link>https://mariamonzon.com/project/adaptive-frame-sampling/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/project/adaptive-frame-sampling/</guid>
      <description>&lt;p&gt;The goal of the project seminar is to implement an adaptive sampling strategy to dynamically
tune the sampling rate of a wearable egocentric camera. The sampling rate will be tuned
based on context measure, i.e., a measure of motion, extracted from the frames.&lt;/p&gt;
&lt;p&gt;The increasing development new media and data acquisition techniques have lead to new
innovative video recording set ups. A clear example of that is the egocentric video, where
a camera on head or on the chest approximates the visual field of the camera wearer.
This new camera setting offers a valuable perspective to understand user&amp;rsquo;s activities and
their context. In this case, the wearable head-mounted egocentric camera set up pursued
a dietary event spotting in a free living condition.&lt;/p&gt;
&lt;p&gt;A main feature of free-living data is a long recording duration. However, this kind of
camera often acquire irrelevant data for the analysis task.
In this work we introduce an adaptive sampling strategy to dynamically tune the sam-
pling rate of a wearable egocentric camera. The adaptive sampling rate is based on a
context measure. It is a motion measure, indicative if the recorded activity might be of
our interest.&lt;/p&gt;
&lt;p&gt;To evaluate the sampling strategy we implemented a computational simulation of resource consumption in terms of energy consumption and memory storage. For this purpose, I designed an analytical energy and memory model, a
nd base our analysis on data extracted from datasheets. Our energy and memory model considers
both sensing components, i.e., the cmos sensor, and processing, i.e., the microcontroller
to process the deep neural network.&lt;/p&gt;
&lt;p&gt;The clear advantage of an adaptive frame rate is saving energy as the camera will not be
acquiring data continuously. 
Benefits from our method will be longer recording sessions, smaller battery employment or smaller
storage space usage. This would help to overcome the limitations of the video recording,
in terms of power consumption, while maintaining acceptable performance. 
Furthermore, less data acquisition will also mean an energy saving in processing.&lt;/p&gt;
&lt;script src= &#39;/js/pdf-js/build/pdf.js&#39;&gt;&lt;/script&gt;&lt;style&gt;
#the-canvas {
  border: 1px solid black;
  direction: ltr;
  width: 100%;
  height: auto;
  display: none;
}

#paginator {
  display: none;
  text-align: center;
  margin-bottom: 10px;
}

#loadingWrapper {
  display: none;
  justify-content: center;
  align-items: center;
  width: 100%;
  height: 350px;
}

#loading {
  display: inline-block;
  width: 50px;
  height: 50px;
  border: 3px solid #d2d0d0;;
  border-radius: 50%;
  border-top-color: #383838;
  animation: spin 1s ease-in-out infinite;
  -webkit-animation: spin 1s ease-in-out infinite;
}

@keyframes spin {
  to { -webkit-transform: rotate(360deg); }
}
@-webkit-keyframes spin {
  to { -webkit-transform: rotate(360deg); }
}
&lt;/style&gt;

&lt;div id=&#34;paginator&#34;&gt;
    &lt;button id=&#34;prev&#34;&gt;Previous&lt;/button&gt;
    &lt;button id=&#34;next&#34;&gt;Next&lt;/button&gt;
    &amp;nbsp; &amp;nbsp;
    &lt;span&gt;Page: &lt;span id=&#34;page_num&#34;&gt;&lt;/span&gt; / &lt;span id=&#34;page_count&#34;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/div&gt;
&lt;div id=&#34;embed-pdf-container&#34;&gt;
    &lt;div id=&#34;loadingWrapper&#34;&gt;
      &lt;div id=&#34;loading&#34;&gt;&lt;/div&gt;
    &lt;/div&gt;
    &lt;canvas id=&#34;the-canvas&#34;&gt;&lt;/canvas&gt;
&lt;/div&gt;

&lt;script type=&#34;text/javascript&#34;&gt;
window.onload = function() {


var url = &#34;https:\/\/mariamonzon.com\/&#34; + &#39;.\/adaptive_sampling_report_pdf\u0027.pdf\u0022&#39;;

var hidePaginator = &#34;&#34; === &#34;true&#34;;
var hideLoader = &#34;&#34; === &#34;true&#34;;
var selectedPageNum = parseInt(&#34;&#34;) || 1;


var pdfjsLib = window[&#39;pdfjs-dist/build/pdf&#39;];


pdfjsLib.GlobalWorkerOptions.workerSrc = &#34;https:\/\/mariamonzon.com\/&#34; + &#39;/js/pdf-js/build/pdf.worker.js&#39;;


var pdfDoc = null,
    pageNum = selectedPageNum,
    pageRendering = false,
    pageNumPending = null,
    scale = 3,
    canvas = document.getElementById(&#39;the-canvas&#39;),
    ctx = canvas.getContext(&#39;2d&#39;),
    paginator = document.getElementById(&#34;paginator&#34;),
    loadingWrapper = document.getElementById(&#39;loadingWrapper&#39;);



showPaginator();
showLoader();



function renderPage(num) {
  pageRendering = true;
  
  pdfDoc.getPage(num).then(function(page) {
    var viewport = page.getViewport({scale: scale});
    canvas.height = viewport.height;
    canvas.width = viewport.width;

    
    var renderContext = {
      canvasContext: ctx,
      viewport: viewport
    };
    var renderTask = page.render(renderContext);

    
    renderTask.promise.then(function() {
      pageRendering = false;
      showContent();
      
      if (pageNumPending !== null) {
        
        renderPage(pageNumPending);
        pageNumPending = null;
      }
    });
  });

  
  document.getElementById(&#39;page_num&#39;).textContent = num;
}



function showContent() {
  loadingWrapper.style.display = &#39;none&#39;;
  canvas.style.display = &#39;block&#39;;
}



function showLoader() {
  if(hideLoader) return
  loadingWrapper.style.display = &#39;flex&#39;;
  canvas.style.display = &#39;none&#39;;
}



function showPaginator() {
  if(hidePaginator) return
  paginator.style.display = &#39;block&#39;;
}



function queueRenderPage(num) {
  if (pageRendering) {
    pageNumPending = num;
  } else {
    renderPage(num);
  }
}



function onPrevPage() {
  if (pageNum &lt;= 1) {
    return;
  }
  pageNum--;
  queueRenderPage(pageNum);
}
document.getElementById(&#39;prev&#39;).addEventListener(&#39;click&#39;, onPrevPage);



function onNextPage() {
  if (pageNum &gt;= pdfDoc.numPages) {
    return;
  }
  pageNum++;
  queueRenderPage(pageNum);
}
document.getElementById(&#39;next&#39;).addEventListener(&#39;click&#39;, onNextPage);



pdfjsLib.getDocument(url).promise.then(function(pdfDoc_) {
  pdfDoc = pdfDoc_;
  var numPages = pdfDoc.numPages;
  document.getElementById(&#39;page_count&#39;).textContent = numPages;
  
  
  if(pageNum &gt; numPages) {
    pageNum = numPages
  }

  
  renderPage(pageNum);
});
}

&lt;/script&gt;

</description>
    </item>
    
    <item>
      <title>ECG-based algorithm for Annotation of Resuscitation Episode</title>
      <link>https://mariamonzon.com/project/ecg-annotation-svm-classifier/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://mariamonzon.com/project/ecg-annotation-svm-classifier/</guid>
      <description>&lt;p&gt;Out-of hospital cardiac arrest (OHCA) is one of the major causes of death in developed countries. Resuscitation guidelines recommend different treatments depending on the heart rhythm of the patient. The objective of this work is to develop a machine learning algorithm based on the ECG signal to automatically label heart rhythms in resuscitation episodes, a key tool for the retrospectively evaluation and improvement of the quality treatment. This work would help to systematise the annotation of databases since manual annotation of rhythms is a time-consuming task which can be an obstacle for handling large data sets.&lt;/p&gt;
&lt;p&gt;The starting point of this project was a database composed of 1631 intervals of 3 seconds taken from a larger database containing OHCA 298 episodes. To review the ECG segments, a graphical interface (GUI) was developed which allows the display of the ECGs classified by the type of arrhythmia: 
asystole (AS), ventricular tachycardia (VT) that degenerates into ventricular fibrillation (VF), pulseless electrical activity (PEA) pulse generating rhythms (PR).&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/egc-svm-annotation/GUI-visualization.png&#34; alt=&#34;GUI-example-visualization&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The database has been processed using a machine learning algorithm and the results obtained using cross-validation. Two classifiers have been developed selecting five features of the ECG, first to identify AS, and then to discriminate organised rhythms (PR and PEA) from ventricular arrhythmias (VT and VF).&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/egc-svm-annotation/ECG-classification-Example.png&#34; alt=&#34;ECG-Visualization&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;These algorithms have been combined to create a three class rhythm classification algorithm. The total accuracy of the final algorithm was 90.9%. A precise algorithm was obtained for the classification of OHCA rhythm into: AS, organised,  and  shockable  rhythms.  This  algorithm  can  be  implemented  to  analyse resuscitation episodes using 3 seconds ECG segments, and could be integrated into new methods for retrospective analysis of OHCA.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://mariamonzon.com/images/egc-svm-annotation/SVM-classification-results.png&#34; alt=&#34;ECG-Visualization&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The results show that it is possible to automatically  interpret  resuscitation  cardiac  rhythm. These types of algorithms can be very useful since they allow an efficient rhythm classification with a minimum level of expert clinician supervision&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
